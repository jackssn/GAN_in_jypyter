{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашний проект на тему CycleGAN [DL School - spring 2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Источник вдохновения: https://github.com/hanyoseob/pytorch-CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи:\n",
    "1. Построить свою архитектуру CycleGAN\n",
    "2. Выбрать задачу для этого инструмента для интереса\n",
    "3. Выбрать задачу для этого инструмента из решенных\n",
    "4. Решить задачу, реализация которой уже есть, используя свою архитектуру и свой pipeline\n",
    "5. Решить свою задачу по аналогии с решенной задачей на наработанных инструментах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1. Реализация решенной задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве задачи-исходника был выбран проект перерисовки яблок в апельсины. За основу реализации взят проект человека с именем **hanyoseob**. Проект масштабный, лежал на гитхабе. Я постарался его максимально упростить и перенести в Jypyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе работы:\n",
    "1. Был скачан датасет apple2orange,\n",
    "2. Созданы 2 сети со стандартными архитектурами из [статьи-первоисточника](https://arxiv.org/pdf/1703.10593.pdf): генератор и дискриминатор\n",
    "3. Приведен в порядок код, который нужен для правильной работы функций тренировки и тестирования сети\n",
    "4. Приведены в порядок функции **train** и **test** для правильной работы в Notebook\n",
    "5. Проведено обучение сети на 90 эпохах\n",
    "6. Проведен тест сети на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and unzip dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Choose dataset to download (if dataset not already downloaded)\n",
    "ds_id = input(\n",
    "    'Choose dataset to download:\\n(1) \\\n",
    "apple2orange\\n(2) cezanne2photo\\n(3) \\\n",
    "horse2zebra\\n(4) monet2photo\\n(5) \\\n",
    "summer2winter_yosemite\\n(6) ukiyoe2photo\\n(7) vangogh2photo\\n')\n",
    "\n",
    "if ds_id == '1':\n",
    "    ds_name = 'apple2orange'\n",
    "elif ds_id == '2':\n",
    "    ds_name = 'cezanne2photo'\n",
    "elif ds_id == '3':\n",
    "    ds_name = 'horse2zebra'\n",
    "elif ds_id == '4':\n",
    "    ds_name = 'monet2photo'\n",
    "elif ds_id == '5':\n",
    "    ds_name = 'summer2winter_yosemite'\n",
    "elif ds_id == '6':\n",
    "    ds_name = 'ukiyoe2photo'\n",
    "elif ds_id == '7':\n",
    "    ds_name = 'vangogh2photo'\n",
    "else:\n",
    "    sys.exit('Incorrect dataset')\n",
    "\n",
    "if not os.path.exists('datasets'):\n",
    "    os.makedirs('datasets')\n",
    "\n",
    "if ds_name not in os.listdir('datasets'):\n",
    "    url = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/' + ds_name + '.zip'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('datasets/%s.zip' % ds_name, 'wb').write(r.content)\n",
    "else:\n",
    "    print('Dataset %s already downloaded' % ds_name)\n",
    "\n",
    "if '%s.zip' % ds_name in os.listdir('datasets'):\n",
    "    unzip_q = input('Want to unzip zip-file? (y/n)\\n')\n",
    "    if unzip_q.lower() == 'y':\n",
    "        with zipfile.ZipFile('datasets/%s.zip' % ds_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall('datasets')\n",
    "        print('Dataset %s unzip successfully' % ds_name)\n",
    "    \n",
    "    remove_q = input('Want to remove zip-file? (y/n)\\n')\n",
    "    if remove_q.lower() == 'y':\n",
    "        os.remove('datasets/%s.zip' % ds_name)\n",
    "        print('Dataset %s removed successfully' % ds_name)\n",
    "\n",
    "print('Files in datasets-dir:', ' | '.join((_ for _ in os.listdir('datasets'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для правильной работы необходимо, чтобы датасеты двух классов были одинакового размера. Удалим лишние картинки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "testA_files = os.listdir('datasets/%s/testA' % ds_name)\n",
    "testB_files = os.listdir('datasets/%s/testB' % ds_name)\n",
    "trainA_files = os.listdir('datasets/%s/trainA' % ds_name)\n",
    "trainB_files = os.listdir('datasets/%s/trainB' % ds_name)\n",
    "\n",
    "diff_train = len(trainA_files) - len(trainB_files)\n",
    "diff_test = len(testA_files) - len(testB_files)\n",
    "\n",
    "ind = -1\n",
    "while diff_train != 0:\n",
    "    ind += 1\n",
    "    if diff_train > 0:\n",
    "        os.remove('datasets/%s/trainA/%s' % (ds_name, trainA_files[ind]))\n",
    "        diff_train -= 1\n",
    "    else:\n",
    "        os.remove('datasets/%s/trainB/%s' % (ds_name, trainB_files[ind]))\n",
    "        diff_train += 1\n",
    "\n",
    "ind = -1\n",
    "while diff_test != 0:\n",
    "    ind += 1\n",
    "    if diff_test > 0:\n",
    "        os.remove('datasets/%s/testA/%s' % (ds_name, testA_files[ind]))\n",
    "        diff_test -= 1\n",
    "    else:\n",
    "        os.remove('datasets/%s/testB/%s' % (ds_name, testB_files[ind]))\n",
    "        diff_test += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set parameters to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_data = ds_name\n",
    "DIRECTION = 'A2B'\n",
    "#DIRECTION = 'B2A'\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "XY_PIX = 256  # size of picture\n",
    "\n",
    "wgt_c_a = 10\n",
    "wgt_c_b = 10\n",
    "wgt_i = 0.5\n",
    "\n",
    "LR_GEN = 0.0002\n",
    "LR_DISC = 0.0002\n",
    "\n",
    "# Params to save pics and weights\n",
    "num_freq_disp = 10  # display every N epochs\n",
    "num_freq_save = 1  # save every N epochs\n",
    "\n",
    "dir_checkpoint = './checkpoint'\n",
    "dir_data = './datasets'\n",
    "dir_log = './log'\n",
    "dir_result = './result'\n",
    "\n",
    "dir_chck = os.path.join(dir_checkpoint, name_data)\n",
    "dir_data_train = os.path.join(dir_data, name_data, 'train')\n",
    "dir_log_train = os.path.join(dir_log, name_data, 'train')\n",
    "dir_data_test = os.path.join(dir_data, name_data, 'test')\n",
    "dir_result_save = os.path.join(dir_result, name_data, 'images')\n",
    "\n",
    "if not os.path.exists(dir_chck):\n",
    "    os.makedirs(dir_chck)\n",
    "if not os.path.exists(dir_log):\n",
    "    os.makedirs(dir_log)\n",
    "if not os.path.exists(dir_result_save):\n",
    "    os.makedirs(dir_result_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import cv2\n",
    "from statistics import mean\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "from matplotlib import rcParams\n",
    "from IPython.display import clear_output\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Build the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ENCODER\n",
    "        # 256 x 256 x 3 -> 128 x 128 x 64\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        # 128 x 128 x 64 -> 64 x 64 x 128\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        # 64 x 64 x 128 -> 32 x 32 x 256\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        # 32 x 32 x 256 -> 16 x 16 x 512\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        # 16 x 16 x 512 -> 8 x 8 x 512\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        # 8 x 8 x 512 -> 4 x 4 x 512\n",
    "        self.enc6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        # 4 x 4 x 512 -> 2 x 2 x 512\n",
    "        self.enc7 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        # 2 x 2 x 512 -> 2 x 2 x 512\n",
    "        self.enc8 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # DECODER\n",
    "        self.dec8 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.dec7 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.dec6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        enc5 = self.enc5(enc4)\n",
    "        enc6 = self.enc6(enc5)\n",
    "        enc7 = self.enc7(enc6)\n",
    "        enc8 = self.enc8(enc7)\n",
    "\n",
    "        dec8 = self.dec8(enc8)\n",
    "        dec7 = self.dec7(torch.cat([enc7, dec8], dim=1))\n",
    "        dec6 = self.dec6(torch.cat([enc6, dec7], dim=1))\n",
    "        dec5 = self.dec5(torch.cat([enc5, dec6], dim=1))\n",
    "        dec4 = self.dec4(torch.cat([enc4, dec5], dim=1))\n",
    "        dec3 = self.dec3(torch.cat([enc3, dec4], dim=1))\n",
    "        dec2 = self.dec2(torch.cat([enc2, dec3], dim=1))\n",
    "        dec1 = self.dec1(torch.cat([enc1, dec2], dim=1))\n",
    "\n",
    "        return dec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for check right architecture\n",
    "#genA = UNet()\n",
    "#torchsummary.summary(genA.to(device), (3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build the Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 256 x 256 x 3 -> 128 x 128 x 64\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        \n",
    "        # 128 x 128 x 64 -> 64 x 64 x 128\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        \n",
    "        # 64 x 64 x 128 -> 32 x 32 x 256\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        \n",
    "        # 32 x 32 x 256 -> 32 x 32 x 512\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        \n",
    "        # 32 x 32 x 512 -> 32 x 32 x 1\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.enc1(x)\n",
    "        x = self.enc2(x)\n",
    "        x = self.enc3(x)\n",
    "        x = self.enc4(x)\n",
    "        x = self.enc5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for check right architecture\n",
    "#discrA = Discriminator()\n",
    "#torchsummary.summary(discrA.to(device), (3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "    \"\"\"Initialize network weights.\n",
    "\n",
    "    Parameters:\n",
    "        net (network)   -- network to be initialized\n",
    "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
    "\n",
    "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
    "    work better for some applications. Feel free to try yourself.\n",
    "    \"\"\"\n",
    "    def init_func(m):  # define the initialization function\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                nn.init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                nn.init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "            nn.init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)  # apply the initialization function <init_func>\n",
    "\n",
    "\n",
    "def init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[0]):\n",
    "    \"\"\"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n",
    "    Parameters:\n",
    "        net (network)      -- the network to be initialized\n",
    "        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        gain (float)       -- scaling factor for normal, xavier and orthogonal.\n",
    "        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n",
    "\n",
    "    Return an initialized network.\n",
    "    \"\"\"\n",
    "    if gpu_ids:\n",
    "        assert(torch.cuda.is_available())\n",
    "        net.to(gpu_ids[0])\n",
    "        net = torch.nn.DataParallel(net, gpu_ids)  # multi-GPUs\n",
    "    init_weights(net, init_type, init_gain=init_gain)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Create class to get data from dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    dataset of image files of the form \n",
    "       stuff<number>_trans.pt\n",
    "       stuff<number>_density.pt\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, direction='A2B', data_type='float32', nch=3, transform=[]):\n",
    "        self.data_dir_a = data_dir + 'A'\n",
    "        self.data_dir_b = data_dir + 'B'\n",
    "        self.transform = transform\n",
    "        self.direction = direction\n",
    "        self.data_type = data_type\n",
    "        self.nch = nch\n",
    "\n",
    "        dataA = [f for f in os.listdir(self.data_dir_a) if f.endswith('.jpg')]\n",
    "        dataA.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "        dataB = [f for f in os.listdir(self.data_dir_b) if f.endswith('.jpg')]\n",
    "        dataB.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "        self.names = (dataA, dataB)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        dataA = plt.imread(os.path.join(self.data_dir_a, self.names[0][index])).squeeze()\n",
    "        dataB = plt.imread(os.path.join(self.data_dir_b, self.names[1][index])).squeeze()\n",
    "\n",
    "        if dataA.dtype == np.uint8:\n",
    "            dataA = dataA / 255.0\n",
    "\n",
    "        if dataB.dtype == np.uint8:\n",
    "            dataB = dataB / 255.0\n",
    "\n",
    "        if len(dataA.shape) == 2:\n",
    "            dataA = np.expand_dims(dataA, axis=2)\n",
    "            dataA = np.tile(dataA, (1, 1, 3))\n",
    "        if len(dataB.shape) == 2:\n",
    "            dataB = np.expand_dims(dataB, axis=2)\n",
    "            dataB = np.tile(dataB, (1, 1, 3))\n",
    "\n",
    "        if self.direction == 'A2B':\n",
    "            data = {'dataA': dataA, 'dataB': dataB}\n",
    "        else:\n",
    "            data = {'dataA': dataB, 'dataB': dataA}\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Create additional functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я скопировал функции для преобразования тензоров и массивов при работе с картинками, так как стандартные библиотеки выдавали ошибки. А здесь все прозрачно и очевидно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    def __call__(self, data):\n",
    "        # Nomalize [0, 1] => [-1, 1]\n",
    "\n",
    "        # for key, value in data:\n",
    "        #     data[key] = 2 * (value / 255) - 1\n",
    "        #\n",
    "        # return data\n",
    "\n",
    "        dataA, dataB = data['dataA'], data['dataB']\n",
    "        dataA = 2 * dataA - 1\n",
    "        dataB = 2 * dataB - 1\n",
    "        return {'dataA': dataA, 'dataB': dataB}\n",
    "\n",
    "class Denormalize(object):\n",
    "    def __call__(self, data):\n",
    "        return (data + 1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Tensor <=> Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Swap color axis because numpy image: H x W x C\n",
    "        #                         torch image: C x H x W\n",
    "\n",
    "        # for key, value in data:\n",
    "        #     data[key] = torch.from_numpy(value.transpose((2, 0, 1)))\n",
    "        #\n",
    "        # return data\n",
    "\n",
    "        dataA, dataB = data['dataA'], data['dataB']\n",
    "\n",
    "        dataA = dataA.transpose((2, 0, 1)).astype(np.float32)\n",
    "        dataB = dataB.transpose((2, 0, 1)).astype(np.float32)\n",
    "        return {'dataA': torch.from_numpy(dataA), 'dataB': torch.from_numpy(dataB)}\n",
    "\n",
    "    \n",
    "class ToNumpy(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Swap color axis because numpy image: H x W x C\n",
    "        #                         torch image: C x H x W\n",
    "\n",
    "        # for key, value in data:\n",
    "        #     data[key] = value.transpose((2, 0, 1)).numpy()\n",
    "        #\n",
    "        # return data\n",
    "\n",
    "        return data.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "        # input, label = data['input'], data['label']\n",
    "        # input = input.transpose((2, 0, 1))\n",
    "        # label = label.transpose((2, 0, 1))\n",
    "        # return {'input': input.detach().numpy(), 'label': label.detach().numpy()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Image transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFlip(object):\n",
    "    def __call__(self, data):\n",
    "        # Random Left or Right Flip\n",
    "\n",
    "        # for key, value in data:\n",
    "        #     data[key] = 2 * (value / 255) - 1\n",
    "        #\n",
    "        # return data\n",
    "        dataA, dataB = data['dataA'], data['dataB']\n",
    "\n",
    "        if np.random.rand() > 0.5:\n",
    "            dataA = np.fliplr(dataA)\n",
    "            dataB = np.fliplr(dataB)\n",
    "\n",
    "        # if np.random.rand() > 0.5:\n",
    "        #     dataA = np.flipud(dataA)\n",
    "        #     dataB = np.flipud(dataB)\n",
    "\n",
    "        return {'dataA': dataA, 'dataB': dataB}\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size\n",
    "  \n",
    "    Args:\n",
    "      output_size (tuple or int): Desired output size.\n",
    "                                  If tuple, output is matched to output_size.\n",
    "                                  If int, smaller of image edges is matched\n",
    "                                  to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "  \n",
    "    def __call__(self, data):\n",
    "        dataA, dataB = data['dataA'], data['dataB']\n",
    "    \n",
    "        h, w = dataA.shape[:2]\n",
    "    \n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "    \n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "    \n",
    "        dataA = transform.resize(dataA, (new_h, new_w))\n",
    "        dataB = transform.resize(dataB, (new_h, new_w))\n",
    "    \n",
    "        return {'dataA': dataA, 'dataB': dataB}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample\n",
    "  \n",
    "    Args:\n",
    "      output_size (tuple or int): Desired output size.\n",
    "                                  If int, square crop is made.\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "  \n",
    "    def __call__(self, data):\n",
    "        dataA, dataB = data['dataA'], data['dataB']\n",
    "    \n",
    "        h, w = dataA.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "    \n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "    \n",
    "        dataA = dataA[top: top + new_h, left: left + new_w]\n",
    "        dataB = dataB[top: top + new_h, left: left + new_w]\n",
    "  \n",
    "        return {'dataA': dataA, 'dataB': dataB}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Function for right work gradients freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
    "    Parameters:\n",
    "        nets (network list)   -- a list of networks\n",
    "        requires_grad (bool)  -- whether the networks require gradients or not\n",
    "    \"\"\"\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save and load functions for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(dir_chck, netG_a2b, netG_b2a, netD_a, netD_b, optimG, optimD, epoch):\n",
    "        if not os.path.exists(dir_chck):\n",
    "            os.makedirs(dir_chck)\n",
    "\n",
    "        torch.save({'netG_a2b': netG_a2b.state_dict(), 'netG_b2a': netG_b2a.state_dict(),\n",
    "                    'netD_a': netD_a.state_dict(), 'netD_b': netD_b.state_dict(),\n",
    "                    'optimG': optimG.state_dict(), 'optimD': optimD.state_dict()},\n",
    "                   '%s/model_epoch%04d.pth' % (dir_chck, epoch))\n",
    "\n",
    "def load(dir_chck, netG_a2b, netG_b2a, netD_a=[], netD_b=[], optimG=[], optimD=[], epoch=[], mode='train'):\n",
    "    if not epoch:\n",
    "        ckpt = os.listdir(dir_chck)\n",
    "        ckpt.sort()\n",
    "        epoch = int(ckpt[-1].split('epoch')[1].split('.pth')[0])\n",
    "\n",
    "    dict_net = torch.load('%s/model_epoch%04d.pth' % (dir_chck, epoch))\n",
    "\n",
    "    print('Loaded %dth network' % epoch)\n",
    "\n",
    "    if mode == 'train':\n",
    "        netG_a2b.load_state_dict(dict_net['netG_a2b'])\n",
    "        netG_b2a.load_state_dict(dict_net['netG_b2a'])\n",
    "        netD_a.load_state_dict(dict_net['netD_a'])\n",
    "        netD_b.load_state_dict(dict_net['netD_b'])\n",
    "        optimG.load_state_dict(dict_net['optimG'])\n",
    "        optimD.load_state_dict(dict_net['optimD'])\n",
    "\n",
    "        return netG_a2b, netG_b2a, netD_a, netD_b, optimG, optimD, epoch\n",
    "\n",
    "    elif mode == 'test':\n",
    "        netG_a2b.load_state_dict(dict_net['netG_a2b'])\n",
    "        netG_b2a.load_state_dict(dict_net['netG_b2a'])\n",
    "\n",
    "        return netG_a2b, netG_b2a, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, mode='train', train_continue=False, num_epoch=EPOCHS, lr_G=LR_GEN, lr_D=LR_DISC, batch_size=BATCH_SIZE, beta1=0.5):\n",
    "\n",
    "    loader_train = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "    num_train = len(dataset_train)\n",
    "\n",
    "    num_batch_train = int((num_train / batch_size) + ((num_train % batch_size) != 0))\n",
    "\n",
    "    ## setup network\n",
    "    netG_a2b = UNet().to(device)\n",
    "    netG_b2a = UNet().to(device)\n",
    "\n",
    "    netD_a = Discriminator().to(device)\n",
    "    netD_b = Discriminator().to(device)\n",
    "\n",
    "    init_net(netG_a2b, init_type='normal', init_gain=0.02)\n",
    "    init_net(netG_b2a, init_type='normal', init_gain=0.02)\n",
    "\n",
    "    init_net(netD_a, init_type='normal', init_gain=0.02)\n",
    "    init_net(netD_b, init_type='normal', init_gain=0.02)\n",
    "\n",
    "    ## setup loss & optimization\n",
    "    fn_Cycle = nn.L1Loss().to(device)   # L1\n",
    "    fn_GAN = nn.BCEWithLogitsLoss().to(device)\n",
    "    fn_Ident = nn.L1Loss().to(device)   # L1\n",
    "\n",
    "    paramsG_a2b = netG_a2b.parameters()\n",
    "    paramsG_b2a = netG_b2a.parameters()\n",
    "    paramsD_a = netD_a.parameters()\n",
    "    paramsD_b = netD_b.parameters()\n",
    "\n",
    "    optimG = torch.optim.Adam(itertools.chain(paramsG_a2b, paramsG_b2a), lr=lr_G, betas=(beta1, 0.999))\n",
    "    optimD = torch.optim.Adam(itertools.chain(paramsD_a, paramsD_b), lr=lr_D, betas=(beta1, 0.999))\n",
    "\n",
    "    # schedG = get_scheduler(optimG, self.opts)\n",
    "    # schedD = get_scheduler(optimD, self.opts)\n",
    "\n",
    "    # schedG = torch.optim.lr_scheduler.ExponentialLR(optimG, gamma=0.9)\n",
    "    # schedD = torch.optim.lr_scheduler.ExponentialLR(optimD, gamma=0.9)\n",
    "\n",
    "    ## load from checkpoints\n",
    "    st_epoch = 0\n",
    "\n",
    "    if train_continue is True:\n",
    "        netG_a2b, netG_b2a, netD_a, netD_b, optimG, optimD, st_epoch = \\\n",
    "            load(dir_chck, netG_a2b, netG_b2a, netD_a, netD_b, optimG, optimD, mode=mode)\n",
    "\n",
    "    ## setup tensorboard\n",
    "    writer_train = SummaryWriter(log_dir=dir_log_train)\n",
    "\n",
    "    for epoch in range(st_epoch + 1, num_epoch + 1):\n",
    "        ## training phase\n",
    "        netG_a2b.train()\n",
    "        netG_b2a.train()\n",
    "        netD_a.train()\n",
    "        netD_b.train()\n",
    "\n",
    "        loss_G_a2b_train = []\n",
    "        loss_G_b2a_train = []\n",
    "        loss_D_a_train = []\n",
    "        loss_D_b_train = []\n",
    "        loss_C_a_train = []\n",
    "        loss_C_b_train = []\n",
    "        loss_I_a_train = []\n",
    "        loss_I_b_train = []\n",
    "\n",
    "        for i, data in enumerate(loader_train, 1):\n",
    "            def should(freq):\n",
    "                return freq > 0 and (i % freq == 0 or i == num_batch_train)\n",
    "\n",
    "            try:\n",
    "                input_a = data['dataA'].to(device)\n",
    "                input_b = data['dataB'].to(device)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "            # forward netG\n",
    "            output_b = netG_a2b(input_a)\n",
    "            output_a = netG_b2a(input_b)\n",
    "\n",
    "            recon_b = netG_a2b(output_a)\n",
    "            recon_a = netG_b2a(output_b)\n",
    "\n",
    "            # backward netD\n",
    "            set_requires_grad([netD_a, netD_b], True)\n",
    "            optimD.zero_grad()\n",
    "\n",
    "            # backward netD_a\n",
    "            pred_real_a = netD_a(input_a)\n",
    "            pred_fake_a = netD_a(output_a.detach())\n",
    "\n",
    "            loss_D_a_real = fn_GAN(pred_real_a, torch.ones_like(pred_real_a))\n",
    "            loss_D_a_fake = fn_GAN(pred_fake_a, torch.zeros_like(pred_fake_a))\n",
    "            loss_D_a = 0.5 * (loss_D_a_real + loss_D_a_fake)\n",
    "\n",
    "            # backward netD_b\n",
    "            pred_real_b = netD_b(input_b)\n",
    "            pred_fake_b = netD_b(output_b.detach())\n",
    "\n",
    "            loss_D_b_real = fn_GAN(pred_real_b, torch.ones_like(pred_real_b))\n",
    "            loss_D_b_fake = fn_GAN(pred_fake_b, torch.zeros_like(pred_fake_b))\n",
    "            loss_D_b = 0.5 * (loss_D_b_real + loss_D_b_fake)\n",
    "\n",
    "            # backward netD\n",
    "            loss_D = loss_D_a + loss_D_b\n",
    "            loss_D.backward()\n",
    "            optimD.step()\n",
    "\n",
    "            # backward netG\n",
    "            set_requires_grad([netD_a, netD_b], False)\n",
    "            optimG.zero_grad()\n",
    "\n",
    "            if wgt_i > 0:\n",
    "                ident_b = netG_a2b(input_b)\n",
    "                ident_a = netG_b2a(input_a)\n",
    "\n",
    "                loss_I_a = fn_Ident(ident_a, input_a)\n",
    "                loss_I_b = fn_Ident(ident_b, input_b)\n",
    "            else:\n",
    "                loss_I_a = 0\n",
    "                loss_I_b = 0\n",
    "\n",
    "            pred_fake_a = netD_a(output_a)\n",
    "            pred_fake_b = netD_b(output_b)\n",
    "\n",
    "            loss_G_a2b = fn_GAN(pred_fake_b, torch.ones_like(pred_fake_b))\n",
    "            loss_G_b2a = fn_GAN(pred_fake_a, torch.ones_like(pred_fake_a))\n",
    "\n",
    "            loss_C_a = fn_Cycle(input_a, recon_a)\n",
    "            loss_C_b = fn_Cycle(input_b, recon_b)\n",
    "\n",
    "            loss_G = (loss_G_a2b + loss_G_b2a) + \\\n",
    "                     (wgt_c_a * loss_C_a + wgt_c_b * loss_C_b) + \\\n",
    "                     (wgt_c_a * loss_I_a + wgt_c_b * loss_I_b) * wgt_i\n",
    "\n",
    "            loss_G.backward()\n",
    "            optimG.step()\n",
    "\n",
    "            # get losses\n",
    "            loss_G_a2b_train += [loss_G_a2b.item()]\n",
    "            loss_G_b2a_train += [loss_G_b2a.item()]\n",
    "\n",
    "            loss_D_a_train += [loss_D_a.item()]\n",
    "            loss_D_b_train += [loss_D_b.item()]\n",
    "\n",
    "            loss_C_a_train += [loss_C_a.item()]\n",
    "            loss_C_b_train += [loss_C_b.item()]\n",
    "\n",
    "            if wgt_i > 0:\n",
    "                loss_I_a_train += [loss_I_a.item()]\n",
    "                loss_I_b_train += [loss_I_b.item()]\n",
    "\n",
    "            print('TRAIN: EPOCH %d: BATCH %04d/%04d: '\n",
    "                  'G_a2b: %.4f G_b2a: %.4f D_a: %.4f D_b: %.4f C_a: %.4f C_b: %.4f I_a: %.4f I_b: %.4f'\n",
    "                  % (epoch, i, num_batch_train,\n",
    "                     mean(loss_G_a2b_train), mean(loss_G_b2a_train),\n",
    "                     mean(loss_D_a_train), mean(loss_D_b_train),\n",
    "                     mean(loss_C_a_train), mean(loss_C_b_train),\n",
    "                     mean(loss_I_a_train), mean(loss_I_b_train)))\n",
    "\n",
    "            if should(num_freq_disp):\n",
    "                ## show output\n",
    "                input_a = transform_inv(input_a)\n",
    "                output_a = transform_inv(output_a)\n",
    "                input_b = transform_inv(input_b)\n",
    "                output_b = transform_inv(output_b)\n",
    "\n",
    "                writer_train.add_images('input_a', input_a, num_batch_train * (epoch - 1) + i, dataformats='NHWC')\n",
    "                writer_train.add_images('output_a', output_a, num_batch_train * (epoch - 1) + i, dataformats='NHWC')\n",
    "                writer_train.add_images('input_b', input_b, num_batch_train * (epoch - 1) + i, dataformats='NHWC')\n",
    "                writer_train.add_images('output_b', output_b, num_batch_train * (epoch - 1) + i, dataformats='NHWC')\n",
    "\n",
    "        writer_train.add_scalar('loss_G_a2b', mean(loss_G_a2b_train), epoch)\n",
    "        writer_train.add_scalar('loss_G_b2a', mean(loss_G_b2a_train), epoch)\n",
    "        writer_train.add_scalar('loss_D_a', mean(loss_D_a_train), epoch)\n",
    "        writer_train.add_scalar('loss_D_b', mean(loss_D_b_train), epoch)\n",
    "        writer_train.add_scalar('loss_C_a', mean(loss_C_a_train), epoch)\n",
    "        writer_train.add_scalar('loss_C_b', mean(loss_C_b_train), epoch)\n",
    "        writer_train.add_scalar('loss_I_a', mean(loss_I_a_train), epoch)\n",
    "        writer_train.add_scalar('loss_I_b', mean(loss_I_b_train), epoch)\n",
    "\n",
    "        # # update scheduler\n",
    "        # # schedG.step()\n",
    "        # # schedD.step()\n",
    "\n",
    "        ## save model weights\n",
    "        if (epoch % num_freq_save) == 0:\n",
    "            save(dir_chck, netG_a2b, netG_b2a, netD_a, netD_b, optimG, optimD, epoch)\n",
    "\n",
    "    writer_train.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, mode='test', batch_size=BATCH_SIZE):\n",
    "    loader_test = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    num_test = len(dataset_test)\n",
    "\n",
    "    num_batch_test = int((num_test / batch_size) + ((num_test % batch_size) != 0))\n",
    "\n",
    "    ## setup network\n",
    "    netG_a2b = UNet().to(device)\n",
    "    netG_b2a = UNet().to(device)\n",
    "\n",
    "    init_net(netG_a2b, init_type='normal', init_gain=0.02)\n",
    "    init_net(netG_b2a, init_type='normal', init_gain=0.02)\n",
    "\n",
    "    ## load from checkpoints\n",
    "    st_epoch = 0\n",
    "\n",
    "    netG_a2b, netG_b2a, st_epoch = load(dir_chck, netG_a2b, netG_b2a, mode=mode)\n",
    "\n",
    "    ## test phase\n",
    "    with torch.no_grad():\n",
    "        netG_a2b.eval()\n",
    "        netG_b2a.eval()\n",
    "        # netG_a2b.train()\n",
    "        # netG_b2a.train()\n",
    "\n",
    "        gen_loss_l1_test = 0\n",
    "        for i, data in enumerate(loader_test, 1):\n",
    "            input_a = data['dataA'].to(device)\n",
    "            input_b = data['dataB'].to(device)\n",
    "            \n",
    "            # forward netG\n",
    "            output_b = netG_a2b(input_a)\n",
    "            output_a = netG_b2a(input_b)\n",
    "\n",
    "            recon_b = netG_a2b(output_a)\n",
    "            recon_a = netG_b2a(output_b)\n",
    "\n",
    "            input_a = transform_inv(input_a)\n",
    "            input_b = transform_inv(input_b)\n",
    "            output_a = transform_inv(output_a)\n",
    "            output_b = transform_inv(output_b)\n",
    "            recon_a = transform_inv(recon_a)\n",
    "            recon_b = transform_inv(recon_b)\n",
    "\n",
    "            for j in range(input_a.shape[0]):\n",
    "                name = batch_size * (i - 1) + j\n",
    "                fileset = {'name': name,\n",
    "                           'input_a': \"%04d-input_a.png\" % name,\n",
    "                           'output_b': \"%04d-output_b.png\" % name,\n",
    "                           'input_b': \"%04d-input_b.png\" % name,\n",
    "                           'output_a': \"%04d-output_a.png\" % name,\n",
    "                           'recon_a': \"%04d-recon_a.png\" % name,\n",
    "                           'recon_b': \"%04d-recon_b.png\" % name}\n",
    "\n",
    "                plt.imsave(os.path.join(dir_result_save, fileset['input_a']), input_a[j, :, :, :].squeeze())\n",
    "                plt.imsave(os.path.join(dir_result_save, fileset['output_b']), output_b[j, :, :, :].squeeze())\n",
    "                plt.imsave(os.path.join(dir_result_save, fileset['input_b']), input_b[j, :, :, :].squeeze())\n",
    "                plt.imsave(os.path.join(dir_result_save, fileset['output_a']), output_a[j, :, :, :].squeeze())\n",
    "                \n",
    "                plt.imsave(os.path.join(dir_result_save, fileset['recon_a']), recon_a[j, :, :, :].squeeze())\n",
    "                plt.imsave(os.path.join(dir_result_save, fileset['recon_b']), recon_b[j, :, :, :].squeeze())\n",
    "\n",
    "                append_index(dir_result, fileset)\n",
    "\n",
    "                print(\"%d / %d\" % (name + 1, num_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Code to create html-result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_index(dir_result, fileset, step=False):\n",
    "    index_path = os.path.join(dir_result, name_data, \"index.html\")\n",
    "    if os.path.exists(index_path):\n",
    "        index = open(index_path, \"a\")\n",
    "    else:\n",
    "        index = open(index_path, \"w\")\n",
    "        index.write(\"<html><body><table><tr>\")\n",
    "        if step:\n",
    "            index.write(\"<th>step</th>\")\n",
    "        for key, value in fileset.items():\n",
    "            index.write(\"<th>%s</th>\" % key)\n",
    "        index.write('</tr>')\n",
    "\n",
    "    # for fileset in filesets:\n",
    "    index.write(\"<tr>\")\n",
    "\n",
    "    if step:\n",
    "        index.write(\"<td>%d</td>\" % fileset[\"step\"])\n",
    "    index.write(\"<td>%s</td>\" % fileset[\"name\"])\n",
    "\n",
    "    del fileset['name']\n",
    "\n",
    "    for key, value in fileset.items():\n",
    "        index.write(\"<td><img src='images/%s'></td>\" % value)\n",
    "\n",
    "    index.write(\"</tr>\")\n",
    "    return index_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train and test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    Normalize(),\n",
    "    RandomFlip(),\n",
    "    Rescale((XY_PIX+30, XY_PIX+30)),\n",
    "    RandomCrop((XY_PIX, XY_PIX)),\n",
    "    ToTensor(),\n",
    "])\n",
    "transform_inv = transforms.Compose([ToNumpy(), Denormalize(),])\n",
    "\n",
    "dataset_train = Dataset(dir_data_train, direction=DIRECTION, transform=transform_train)\n",
    "\n",
    "train_cont=False\n",
    "if os.listdir(dir_chck):\n",
    "    train_cont=True\n",
    "train(dataset=dataset_train, train_continue=train_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([Normalize(), ToTensor()])\n",
    "transform_inv = transforms.Compose([ToNumpy(), Denormalize()])\n",
    "\n",
    "dataset_test = Dataset(dir_data_test, transform=transform_test)\n",
    "\n",
    "test(dataset=dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. View results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты работы сети можно посмотреть в файле `result/<name_ds>/index.html`\n",
    "\n",
    "`input a | output b | input b | output a | recon a | recon b`<br>\n",
    "`(apple to orange)  | (orange to apple)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также для просмотра Лоссов удобен инструмент **tensorboard**, в который с определенным интервалом записываются лоссы и сохраняет картинки.\n",
    "\n",
    "Для его запуска нужно в командной строке ввести следующий код: `tensorboard --logdir ./log/<name_ds> --port 7777`\n",
    "\n",
    "После запуска по адресу http://localhost:7777 станет доступен Dashboard, на котором во вкладке [scalars](http://localhost:7777/#scalars) находятся лоссы, а во вкладке [images](http://localhost:7777/#images) — картинки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2. Реализация своей задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вдохновение для задачи по интересу было получено из [этой статьи](https://towardsdatascience.com/turning-fortnite-into-pubg-with-deep-learning-cyclegan-2f9d339dcdb0). И готовая реализация на KERAS есть [здесь](https://github.com/bendangnuksung/fortnite-pubg)\n",
    "\n",
    "Для себя я выбрал трансформацию игры CS:GO в OverWatch.\n",
    "\n",
    "Для получения необходимых картинок было скачано несколько роликов игр с youtube, из видео получены картинки и вручную удалено лишнее (заставки, менюшки и тд)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Код для перевода видео в фото"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame(vidcap, sec, start_cnt, dir_dir):\n",
    "    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
    "    hasFrames, image = vidcap.read()\n",
    "    if hasFrames:\n",
    "        cv2.imwrite(\"%s/image%s.jpg\" % (dir_dir, str(count+start_cnt)), image)     # save frame as JPG file\n",
    "    return vidcap, hasFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Было несколько видеороликов, которые должны лежать соответственно в папках **video_cs** и **video_ow** и иметь название **N_video.mp4**, где N - номер по порядку (начиная с 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_photos_from_video(video_dir_name, photo_dir_name, n, frame_rate=0.25):\n",
    "    start_cnt = 0\n",
    "    count = 0\n",
    "    success = 1\n",
    "    for i in range(n):\n",
    "        print(i, \"from\", n)\n",
    "        sec = 0\n",
    "        vidcap = cv2.VideoCapture('%s/%s_video.mp4' % (video_dir_name, i))\n",
    "        fps = vidcap.get(cv2.CAP_PROP_FPS) \n",
    "        length_sec = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT) / fps)\n",
    "        while sec <= length_sec:\n",
    "            count = count + 1\n",
    "            if count % 50 == 0:\n",
    "                print(count, \"/\", int(length_sec/frame_rate))\n",
    "            sec += frame_rate\n",
    "            sec = round(sec, 2)\n",
    "            vidcap, success = get_frame(vidcap, sec, start_cnt, photo_dir_name)\n",
    "        start_cnt = count\n",
    "    print(\"Success!\", video_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_photos_from_video(video_dir_name='video_cs', photo_dir_name='photo_cs', n=6)\n",
    "#create_photos_from_video(video_dir_name='video_ow', photo_dir_name='photo_ow', n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После получения изображений перенес их в папку `datasets/overwatch_to_cs` по аналогии с другими датасетами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Подготовка к обучению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно скачать мой датасет, используя следующий код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "ds_name = 'overwatch2csgo'\n",
    "\n",
    "if ds_name not in os.listdir('datasets'):\n",
    "    url = 'http://jackssn.com/datasets/%s.zip' % ds_name\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('datasets/%s.zip' % ds_name, 'wb').write(r.content)\n",
    "else:\n",
    "    print('Dataset %s already downloaded' % ds_name)\n",
    "\n",
    "if '%s.zip' % ds_name in os.listdir('datasets'):\n",
    "    unzip_q = input('Want to unzip zip-file? (y/n)\\n')\n",
    "    if unzip_q.lower() == 'y':\n",
    "        with zipfile.ZipFile('datasets/%s.zip' % ds_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall('datasets')\n",
    "        print('Dataset %s unzip successfully' % ds_name)\n",
    "    \n",
    "    remove_q = input('Want to remove zip-file? (y/n)\\n')\n",
    "    if remove_q.lower() == 'y':\n",
    "        os.remove('datasets/%s.zip' % ds_name)\n",
    "        print('Dataset %s removed successfully' % ds_name)\n",
    "\n",
    "print('Files in datasets-dir:', ' | '.join((_ for _ in os.listdir('datasets'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "ds_name = 'overwatch2csgo'\n",
    "name_data = ds_name\n",
    "DIRECTION = 'A2B'\n",
    "#DIRECTION = 'B2A'\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "XY_PIX = 256  # size of picture\n",
    "\n",
    "wgt_c_a = 10\n",
    "wgt_c_b = 10\n",
    "wgt_i = 0.5\n",
    "\n",
    "LR_GEN = 0.0002\n",
    "LR_DISC = 0.0002\n",
    "\n",
    "# Params to save pics and weights\n",
    "num_freq_disp = 10  # display every N epochs\n",
    "num_freq_save = 1  # save every N epochs\n",
    "\n",
    "dir_checkpoint = './checkpoint'\n",
    "dir_data = './datasets'\n",
    "dir_log = './log'\n",
    "dir_result = './result'\n",
    "\n",
    "dir_chck = os.path.join(dir_checkpoint, name_data)\n",
    "dir_data_train = os.path.join(dir_data, name_data, 'train')\n",
    "dir_log_train = os.path.join(dir_log, name_data, 'train')\n",
    "dir_data_test = os.path.join(dir_data, name_data, 'test')\n",
    "dir_result_save = os.path.join(dir_result, name_data, 'images')\n",
    "\n",
    "if not os.path.exists(dir_chck):\n",
    "    os.makedirs(dir_chck)\n",
    "if not os.path.exists(dir_log):\n",
    "    os.makedirs(dir_log)\n",
    "if not os.path.exists(dir_result_save):\n",
    "    os.makedirs(dir_result_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Запуск обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если память видеокарты переполнена, нужно перезагрузить кернел и еще раз определить модели и функции, а затем запускать обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    Normalize(),\n",
    "    RandomFlip(),\n",
    "    Rescale((XY_PIX+30, XY_PIX+30)),\n",
    "    RandomCrop((XY_PIX, XY_PIX)),\n",
    "    ToTensor(),\n",
    "])\n",
    "transform_inv = transforms.Compose([ToNumpy(), Denormalize(),])\n",
    "\n",
    "dataset_train = Dataset(dir_data_train, direction=DIRECTION, transform=transform_train)\n",
    "\n",
    "train_cont=False\n",
    "if os.listdir(dir_chck):\n",
    "    train_cont=True\n",
    "train(dataset=dataset_train, train_continue=train_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([Normalize(), Rescale((XY_PIX+30, XY_PIX+30)),RandomCrop((XY_PIX, XY_PIX)),ToTensor()])\n",
    "transform_inv = transforms.Compose([ToNumpy(), Denormalize()])\n",
    "\n",
    "dataset_test = Dataset(dir_data_test, transform=transform_test)\n",
    "\n",
    "test(dataset=dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Результаты работы модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты работы сети можно посмотреть в файле `result/<name_ds>/index.html`\n",
    "\n",
    "`input a | output b | input b | output a | recon a | recon b`<br>\n",
    "`(apple to orange)  | (orange to apple)`\n",
    "\n",
    "Также результаты можно посмотреть по ссылке: https://jackssn.com/result/overwatch2csgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
